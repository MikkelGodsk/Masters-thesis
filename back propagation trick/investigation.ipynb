{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bbZ79XzzB4K"
      },
      "source": [
        "# Back propagation trick\n",
        "Experiments here require torch, preferably with cuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CF5_0ZzhzB4M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from copy import deepcopy\n",
        "from pickle import dump\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq34PkD3zB4O"
      },
      "source": [
        "## Manual back propagation implementation\n",
        "Sanity check: How exactly do we compute the gradient across a single dense layer.\n",
        "\n",
        "**Key take away:** The gradient updates to the weights are simply just outer-products. They can be computed using only the \"error signal\" and the activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxmjUsfuzB4P",
        "outputId": "ebecd4e9-d534-4a67-9248-0916a8a2cc42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight computation: torch.Size([3, 5]) @ torch.Size([5, 10]) = torch.Size([3, 10])\n",
            "Bias computation: torch.Size([3])\n",
            "Pass-through computation: torch.Size([5, 3]) @ torch.Size([3, 10]) = torch.Size([5, 10])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "n_inputs = 10\n",
        "n_hidden_units = 3\n",
        "\n",
        "for i in range(10000):\n",
        "  layer = nn.Linear(n_inputs, n_hidden_units, bias=True)\n",
        "  z0 = torch.randn([batch_size,n_inputs], requires_grad=True, dtype=torch.float32)  # 5 batches, 5 inputs\n",
        "  z1 = layer(z0)\n",
        "  z2 = F.relu(z1)\n",
        "\n",
        "  upstream_grad = torch.randn([batch_size, n_hidden_units], dtype=torch.float32)    # 5 batches, 3 n_hidden_units\n",
        "  z2.backward(upstream_grad, inputs=[z0]+list(layer.parameters()))\n",
        "\n",
        "  relu_grad = (z1>0).float()\n",
        "\n",
        "  error_signal = relu_grad*upstream_grad          # (batch_size, n_hidden_units)\n",
        "\n",
        "  # Theoretical layer weight grad\n",
        "  weight_grad = error_signal.T @ z0         # The gradient is a rank-1 sum (outer product):    (n_hidden_units, batch_size) @ (batch_size, n_inputs = (n_hidden_units, n_inputs)\n",
        "  assert torch.all(layer.weight.grad == weight_grad)\n",
        "\n",
        "  # Theoretical bias grad\n",
        "  bias_grad = error_signal.sum(axis=0)      # The gradient is a sum over the batch axis:  (n_hidden_units,)\n",
        "  assert torch.all(layer.bias.grad == bias_grad)\n",
        "\n",
        "  # Theoretical \"pass-through gradient\"\n",
        "  pass_through_grad = error_signal @ layer.weight # The gradient is a rank-1 sum:    (batch_size, n_hidden_units) @ (n_hidden_units, n_inputs) = (batch_size, n_inputs)\n",
        "  assert torch.all(z0.grad == pass_through_grad)\n",
        "\n",
        "\n",
        "print(f\"Weight computation: {error_signal.T.shape} @ {z0.shape} = {weight_grad.shape}\")\n",
        "print(f\"Bias computation: {error_signal.sum(axis=0).shape}\")\n",
        "print(f\"Pass-through computation: {error_signal.shape} @ {layer.weight.shape} = {pass_through_grad.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RszfbOzvzB4Q"
      },
      "source": [
        "## Testing the fused approach\n",
        "Do the two approaches produce exactly the same result during training? If Adam performs some sort of Gradient normalization over the entire network, then no. But it turns out it doesn't..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6tN9ObhzB4Q",
        "outputId": "690420db-0a1b-4276-b563-d7138ad409b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "class ANN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.dense_1 = nn.LazyLinear(50)\n",
        "    self.dense_2 = nn.LazyLinear(50)\n",
        "    self.dense_3 = nn.LazyLinear(1)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters())\n",
        "\n",
        "  def forward(self, x):\n",
        "    z0 = x\n",
        "    z1 = F.relu(self.dense_1(z0))\n",
        "    z2 = F.relu(self.dense_2(z1))\n",
        "    z3 = self.dense_3(z2)\n",
        "    return z3\n",
        "\n",
        "  def backward(self, loss):\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "class MemoryEfficientCopyANN(nn.Module):\n",
        "  def __init__(self, other):\n",
        "    super().__init__()\n",
        "    self.dense_1 = deepcopy(other.dense_1) #nn.LazyLinear(50)\n",
        "    self.dense_2 = deepcopy(other.dense_2) #nn.LazyLinear(50)\n",
        "    self.dense_3 = deepcopy(other.dense_3) #nn.LazyLinear(1)\n",
        "    self.dense_1_optim = torch.optim.Adam(self.dense_1.parameters())\n",
        "    self.dense_2_optim = torch.optim.Adam(self.dense_2.parameters())\n",
        "    self.dense_3_optim = torch.optim.Adam(self.dense_3.parameters())\n",
        "    self.activations = []\n",
        "\n",
        "  def forward(self, x):\n",
        "    z0 = x\n",
        "    z1 = F.relu(self.dense_1(z0))\n",
        "    z2 = F.relu(self.dense_2(z1))\n",
        "    z3 = self.dense_3(z2)\n",
        "    self.activations = [z0,z1,z2,z3]   # Store references to tensors\n",
        "    return z3\n",
        "\n",
        "  def backward(self, loss):\n",
        "    z0 = self.activations[0]\n",
        "    z1 = self.activations[1]\n",
        "    z2 = self.activations[2]\n",
        "    z3 = self.activations[3]\n",
        "    #[z0,z1,z2,z3] = self.activations\n",
        "    self.activations = []\n",
        "\n",
        "    # Target error\n",
        "    loss.backward(inputs=[z3], retain_graph=True)\n",
        "    delta_L = z3.grad\n",
        "\n",
        "    # Backpropagation with in-place optimization application and deletion\n",
        "    z3.backward(delta_L, inputs=[z2]+list(self.dense_3.parameters()), retain_graph=True)\n",
        "    delta_3 = z2.grad\n",
        "    self.dense_3_optim.step()\n",
        "    self.dense_3_optim.zero_grad(set_to_none=True)\n",
        "    assert self.dense_3.weight.grad is None\n",
        "    assert self.dense_3.bias.grad is None\n",
        "    del delta_L\n",
        "    del z3\n",
        "\n",
        "    z2.backward(delta_3, inputs=[z1]+list(self.dense_2.parameters()), retain_graph=True)\n",
        "    delta_2 = z1.grad\n",
        "    self.dense_2_optim.step()\n",
        "    self.dense_2_optim.zero_grad(set_to_none=True)\n",
        "    assert self.dense_2.weight.grad is None\n",
        "    assert self.dense_2.bias.grad is None\n",
        "    del delta_3\n",
        "    del z2\n",
        "\n",
        "    z1.backward(delta_2, inputs=list(self.dense_1.parameters()), retain_graph=False)\n",
        "    self.dense_1_optim.step()\n",
        "    self.dense_1_optim.zero_grad(set_to_none=True)\n",
        "    assert self.dense_1.weight.grad is None\n",
        "    assert self.dense_1.bias.grad is None\n",
        "    del delta_2\n",
        "    del z1\n",
        "\n",
        "\n",
        "class DummyOptimizer(torch.optim.Optimizer):\n",
        "  \"\"\" A dummy optimizer that does nothing. Used for testing.\"\"\"\n",
        "  def __init__(self, params):\n",
        "    super().__init__(params, {})\n",
        "  def step(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "def net_eq(self, other):   # Overloading __eq__ gives issues\n",
        "  \"\"\"\n",
        "    Defines two networks as the same if all their parameters are the same\n",
        "    regardless of which class they are...\n",
        "  \"\"\"\n",
        "  for s_param, o_param in zip(self.parameters(), other.parameters()):\n",
        "    if not torch.all(s_param == o_param):\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  # Initialize networks\n",
        "  loss_fun = nn.MSELoss()\n",
        "  x = torch.randn((5,50)).to(device)    # Batch of 5, 50 inputs\n",
        "  net1 = ANN().to(device)\n",
        "  pred1 = net1(x)  # Initialize weights\n",
        "  net2 = MemoryEfficientCopyANN(net1).to(device) # They get the same parameters\n",
        "  net2_err = MemoryEfficientCopyANN(net1).to(device)\n",
        "  net2_err.dense_1_optim = DummyOptimizer(net2_err.dense_1.parameters())   # Use this one to check that we did not just reference the same parameter matrix, but that they're truly different objects...\n",
        "  assert net_eq(net1, net2)\n",
        "  assert net_eq(net1, net2_err)\n",
        "\n",
        "  for j in range(100):\n",
        "    x = torch.randn((5,50)).to(device)\n",
        "    y = torch.randn([5,1]).to(device)\n",
        "\n",
        "    # Inference and backward\n",
        "    pred1 = net1(x)\n",
        "    loss1 = loss_fun(pred1, y)\n",
        "    net1.backward(loss1)\n",
        "\n",
        "    pred2 = net2(x)\n",
        "    loss2 = loss_fun(pred2, y)\n",
        "    net2.backward(loss2)\n",
        "\n",
        "    pred2_err = net2_err(x)\n",
        "    loss2_err = loss_fun(pred2_err, y)\n",
        "    net2_err.backward(loss2_err)\n",
        "\n",
        "    # See if their parameters are equal\n",
        "    assert net_eq(net1, net2)\n",
        "    assert not net_eq(net1, net2_err)   # They should not be equal, as we used a dummy optimizer for net2_err... This proves that we did not just reference the same parameter matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDRJnrDezB4R"
      },
      "source": [
        "## Using the PyTorch hooks implementation\n",
        "Source: https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sfaV7eY9zB4S"
      },
      "outputs": [],
      "source": [
        "class MemoryEfficientANNWithHooks(nn.Module):\n",
        "  # Using this implementation: https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.dense_1 = nn.Linear(50, 50)\n",
        "    self.dense_2 = nn.Linear(50, 50)\n",
        "    self.dense_3 = nn.Linear(50, 1)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters())\n",
        "    self.setup_optimizer()\n",
        "\n",
        "  def _optimizer_hook(self, param, optimizer):\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  def setup_optimizer(self):\n",
        "    self.optimizer_list = [(p, torch.optim.Adam([p])) for p in self.parameters()]\n",
        "    for p, optimizer in self.optimizer_list:\n",
        "      hook = lambda param: self._optimizer_hook(param, optimizer)\n",
        "      p.register_post_accumulate_grad_hook(hook)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z0 = x\n",
        "    z1 = F.relu(self.dense_1(z0))\n",
        "    z2 = F.relu(self.dense_2(z1))\n",
        "    z3 = self.dense_3(z2)\n",
        "    return z3\n",
        "\n",
        "  def backward(self, loss):\n",
        "    loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532p9rsZzB4S"
      },
      "source": [
        "# Timing the methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9N89vKbzB4S",
        "outputId": "8749ca97-1b02-4cdb-fa14-41dbd36f2fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timing experiment: net 1\n",
            "1.5 s ± 99.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "\n",
            "Timing experiment: net 2\n",
            "2.99 s ± 124 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "\n",
            "Timing experiment: net 3\n",
            "2.43 s ± 195 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "#%timeit <statement>\n",
        "x = torch.randn((5,50)).to(device)\n",
        "net1 = ANN().to(device)\n",
        "pred1 = net1(x)  # Initialize weights\n",
        "net2 = MemoryEfficientCopyANN(net1).to(device) # They get the same parameters\n",
        "net3 = MemoryEfficientANNWithHooks().to(device)\n",
        "\n",
        "\n",
        "def experiment(net, max_iter=1000):\n",
        "  loss_fun = nn.MSELoss()\n",
        "  for j in range(max_iter):\n",
        "    # Sample random datapoint\n",
        "    x = torch.randn((5,50)).to(device)\n",
        "    y = torch.randn([5,1]).to(device)\n",
        "\n",
        "    # Do inference\n",
        "    pred = net(x)\n",
        "    loss = loss_fun(pred, y)\n",
        "    net.backward(loss)\n",
        "\n",
        "\n",
        "print(\"Timing experiment: net 1\")\n",
        "%timeit experiment(net1)\n",
        "print(\"\\nTiming experiment: net 2\")\n",
        "%timeit experiment(net2)\n",
        "print(\"\\nTiming experiment: net 3\")\n",
        "%timeit experiment(net3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZM51WM0zB4T"
      },
      "source": [
        "## Memory profiling of all 3 approaches\n",
        "Run this in Google Colab... Then download the files and upload them to https://pytorch.org/memory_viz for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5jV6JcfwzB4T"
      },
      "outputs": [],
      "source": [
        "TIME_FORMAT_STR: str = \"%b_%d_%H_%M_%S\"\n",
        "\n",
        "def experiment_memory_profiling(net, max_iter=1000):\n",
        "  torch.cuda.empty_cache()\n",
        "  loss_fun = nn.MSELoss()\n",
        "\n",
        "  # tell CUDA to start recording memory allocations\n",
        "  torch.cuda.memory._record_memory_history(enabled='all')\n",
        "  for j in range(max_iter):\n",
        "    # Sample random datapoints\n",
        "    x = torch.randn((5,50)).cuda()\n",
        "    y = torch.randn([5,1]).cuda()\n",
        "\n",
        "    # Do inference + backward\n",
        "    pred = net(x)\n",
        "    loss = loss_fun(pred, y)\n",
        "    net.backward(loss)\n",
        "\n",
        "  # save a snapshot of the memory allocations\n",
        "  timestamp = datetime.now().strftime(TIME_FORMAT_STR)\n",
        "  file_prefix = f\"{timestamp}\"\n",
        "\n",
        "  s = torch.cuda.memory._snapshot()\n",
        "  with open(f\"{file_prefix}.pickle\", \"wb\") as f:\n",
        "      dump(s, f)\n",
        "\n",
        "  # tell CUDA to stop recording memory allocations now\n",
        "  torch.cuda.memory._record_memory_history(enabled=None)\n",
        "\n",
        "experiment_memory_profiling(net1.cuda())\n",
        "experiment_memory_profiling(net2.cuda())\n",
        "experiment_memory_profiling(net3.cuda())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Aed5iJzB4U"
      },
      "source": [
        "## Paper on this\n",
        "https://arxiv.org/pdf/2306.09782.pdf"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}