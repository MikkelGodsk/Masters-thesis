{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation trick\n",
    "Experiments here require torch, preferably with cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from pickle import dump\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual back propagation implementation\n",
    "Sanity check: How exactly do we compute the gradient across a single dense layer.\n",
    "\n",
    "**Key take away:** The gradient updates to the weights are simply just outer-products. They can be computed using only the \"error signal\" and the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight computation: torch.Size([3, 5]) @ torch.Size([5, 10]) = torch.Size([3, 10])\n",
      "Bias computation: torch.Size([3])\n",
      "Pass-through computation: torch.Size([5, 3]) @ torch.Size([3, 10]) = torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "n_inputs = 10\n",
    "n_hidden_units = 3\n",
    "\n",
    "for i in range(10000):\n",
    "  layer = nn.Linear(n_inputs, n_hidden_units, bias=True)\n",
    "  z0 = torch.randn([batch_size,n_inputs], requires_grad=True, dtype=torch.float32)  # 5 batches, 5 inputs\n",
    "  z1 = layer(z0)\n",
    "  z2 = F.relu(z1)\n",
    "\n",
    "  upstream_grad = torch.randn([batch_size, n_hidden_units], dtype=torch.float32)          # 5 batches, 3 n_hidden_units\n",
    "  z2.backward(upstream_grad, inputs=[z0]+list(layer.parameters()))\n",
    "\n",
    "  relu_grad = (z1>0).float()\n",
    "\n",
    "  error_signal = relu_grad*upstream_grad          # (batch_size, n_hidden_units)\n",
    "\n",
    "  # Theoretical layer weight grad\n",
    "  weight_grad = error_signal.T @ z0         # The gradient is a rank-1 sum (outer product):    (n_hidden_units, batch_size) @ (batch_size, n_inputs = (n_hidden_units, n_inputs)\n",
    "  assert torch.all(layer.weight.grad == weight_grad)\n",
    "\n",
    "  # Theoretical bias grad\n",
    "  bias_grad = error_signal.sum(axis=0)      # The gradient is a sum over the batch axis:  (n_hidden_units,)\n",
    "  assert torch.all(layer.bias.grad == bias_grad)\n",
    "\n",
    "  # Theoretical \"pass-through gradient\"\n",
    "  pass_through_grad = error_signal @ layer.weight # The gradient is a rank-1 sum:    (batch_size, n_hidden_units) @ (n_hidden_units, n_inputs) = (batch_size, n_inputs)\n",
    "  assert torch.all(z0.grad == pass_through_grad)\n",
    "\n",
    "\n",
    "print(f\"Weight computation: {error_signal.T.shape} @ {z0.shape} = {weight_grad.shape}\")\n",
    "print(f\"Bias computation: {error_signal.sum(axis=0).shape}\")\n",
    "print(f\"Pass-through computation: {error_signal.shape} @ {layer.weight.shape} = {pass_through_grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the fused approach\n",
    "Do the two approaches produce exactly the same result during training? If Adam performs some sort of Gradient normalization over the entire network, then no. But it turns out it doesn't..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "d:\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class ANN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.dense_1 = nn.LazyLinear(50)\n",
    "    self.dense_2 = nn.LazyLinear(50)\n",
    "    self.dense_3 = nn.LazyLinear(1)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "  def forward(self, x):\n",
    "    z0 = x\n",
    "    z1 = F.relu(self.dense_1(z0))\n",
    "    z2 = F.relu(self.dense_2(z1))\n",
    "    z3 = self.dense_3(z2)\n",
    "    return z3\n",
    "\n",
    "  def backward(self, loss):\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "class MemoryEfficientCopyANN(nn.Module):\n",
    "  def __init__(self, other):\n",
    "    super().__init__()\n",
    "    self.dense_1 = deepcopy(other.dense_1) #nn.LazyLinear(50)\n",
    "    self.dense_2 = deepcopy(other.dense_2) #nn.LazyLinear(50)\n",
    "    self.dense_3 = deepcopy(other.dense_3) #nn.LazyLinear(1)\n",
    "    self.dense_1_optim = torch.optim.Adam(self.dense_1.parameters())\n",
    "    self.dense_2_optim = torch.optim.Adam(self.dense_2.parameters())\n",
    "    self.dense_3_optim = torch.optim.Adam(self.dense_3.parameters())\n",
    "    self.activations = []\n",
    "\n",
    "  def forward(self, x):\n",
    "    z0 = x\n",
    "    z1 = F.relu(self.dense_1(z0))\n",
    "    z2 = F.relu(self.dense_2(z1))\n",
    "    z3 = self.dense_3(z2)\n",
    "    self.activations = [z0,z1,z2,z3]   # Store references to tensors\n",
    "    return z3\n",
    "\n",
    "  def backward(self, loss):\n",
    "    z0 = self.activations[0]\n",
    "    z1 = self.activations[1]\n",
    "    z2 = self.activations[2]\n",
    "    z3 = self.activations[3]\n",
    "    #[z0,z1,z2,z3] = self.activations\n",
    "    self.activations = []\n",
    "\n",
    "    # Target error\n",
    "    loss.backward(inputs=[z3], retain_graph=True)\n",
    "    delta_L = z3.grad\n",
    "\n",
    "    # Backpropagation with in-place optimization application and deletion\n",
    "    z3.backward(delta_L, inputs=[z2]+list(self.dense_3.parameters()), retain_graph=True)\n",
    "    delta_3 = z2.grad\n",
    "    self.dense_3_optim.step()\n",
    "    self.dense_3_optim.zero_grad()\n",
    "    assert self.dense_3.weight.grad is None\n",
    "    assert self.dense_3.bias.grad is None\n",
    "    del delta_L\n",
    "    del z3\n",
    "\n",
    "    z2.backward(delta_3, inputs=[z1]+list(self.dense_2.parameters()), retain_graph=True)\n",
    "    delta_2 = z1.grad\n",
    "    self.dense_2_optim.step()\n",
    "    self.dense_2_optim.zero_grad()\n",
    "    assert self.dense_2.weight.grad is None\n",
    "    assert self.dense_2.bias.grad is None\n",
    "    del delta_3\n",
    "    del z2\n",
    "\n",
    "    z1.backward(delta_2, inputs=list(self.dense_1.parameters()), retain_graph=False)\n",
    "    self.dense_1_optim.step()\n",
    "    self.dense_1_optim.zero_grad()\n",
    "    assert self.dense_1.weight.grad is None\n",
    "    assert self.dense_1.bias.grad is None\n",
    "    del delta_2\n",
    "    del z1\n",
    "\n",
    "\n",
    "def net_eq(self, other):   # Overloading __eq__ gives issues\n",
    "  \"\"\"\n",
    "    Defines two networks as the same if all their parameters are the same\n",
    "    regardless of which class they are...\n",
    "  \"\"\"\n",
    "  for s_param, o_param in zip(self.parameters(), other.parameters()):\n",
    "    if not torch.all(s_param == o_param):\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "  # Initialize networks\n",
    "  loss_fun = nn.MSELoss()\n",
    "  x = torch.randn((5,50)).to(device)    # Batch of 5, 50 inputs\n",
    "  net1 = ANN().to(device)\n",
    "  pred1 = net1(x)  # Initialize weights\n",
    "  net2 = MemoryEfficientCopyANN(net1).to(device) # They get the same parameters\n",
    "  assert net_eq(net1, net2)\n",
    "\n",
    "  for j in range(10):\n",
    "    x = torch.randn((5,50)).to(device)\n",
    "    y = torch.randn([5,1]).to(device)\n",
    "\n",
    "    # Inference and backward\n",
    "    pred1 = net1(x)\n",
    "    loss1 = loss_fun(pred1, y)\n",
    "    net1.backward(loss1)\n",
    "\n",
    "    pred2 = net2(x)\n",
    "    loss2 = loss_fun(pred2, y)\n",
    "    net2.backward(loss2)\n",
    "\n",
    "    # See if their parameters are equal\n",
    "    assert net_eq(net1, net2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the PyTorch hooks implementation\n",
    "Source: https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientANNWithHooks(nn.Module):\n",
    "  # Using this implementation: https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.dense_1 = nn.Linear(50, 50)\n",
    "    self.dense_2 = nn.Linear(50, 50)\n",
    "    self.dense_3 = nn.Linear(50, 1)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters())\n",
    "    self.setup_optimizer()\n",
    "  \n",
    "  def _optimizer_hook(self, param, optimizer):\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  def setup_optimizer(self):\n",
    "    self.optimizer_list = [(p, torch.optim.Adam([p])) for p in self.parameters()]\n",
    "    for p, optimizer in self.optimizer_list:\n",
    "      hook = lambda param: self._optimizer_hook(param, optimizer)\n",
    "      p.register_post_accumulate_grad_hook(hook)\n",
    "\n",
    "  def forward(self, x):\n",
    "    z0 = x\n",
    "    z1 = F.relu(self.dense_1(z0))\n",
    "    z2 = F.relu(self.dense_2(z1))\n",
    "    z3 = self.dense_3(z2)\n",
    "    return z3\n",
    "  \n",
    "  def backward(self, loss):\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing experiment: net 1\n",
      "1.71 s ± 109 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Timing experiment: net 2\n",
      "3.68 s ± 226 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Timing experiment: net 3\n",
      "3.33 s ± 184 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "#%timeit <statement>\n",
    "x = torch.randn((5,50)).to(device)\n",
    "net1 = ANN().to(device)\n",
    "pred1 = net1(x)  # Initialize weights\n",
    "net2 = MemoryEfficientCopyANN(net1).to(device) # They get the same parameters\n",
    "net3 = MemoryEfficientANNWithHooks().to(device)\n",
    "\n",
    "\n",
    "def experiment(net, max_iter=1000):\n",
    "  loss_fun = nn.MSELoss()\n",
    "  for j in range(max_iter):\n",
    "    # Sample random datapoint\n",
    "    x = torch.randn((5,50)).to(device)\n",
    "    y = torch.randn([5,1]).to(device)\n",
    "\n",
    "    # Do inference\n",
    "    pred = net(x)\n",
    "    loss = loss_fun(pred, y)\n",
    "    net.backward(loss)\n",
    "\n",
    "\n",
    "print(\"Timing experiment: net 1\")\n",
    "%timeit experiment(net1)\n",
    "print(\"\\nTiming experiment: net 2\")\n",
    "%timeit experiment(net2)\n",
    "print(\"\\nTiming experiment: net 3\")\n",
    "%timeit experiment(net3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory profiling of all 3 approaches\n",
    "Run this in Google Colab instead... Then download the files and upload them to https://pytorch.org/memory_viz for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "record_context_cpp is not support on non-linux non-x86_64 platforms",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[38;5;66;03m# tell CUDA to stop recording memory allocations now\u001b[39;00m\n\u001b[0;32m     27\u001b[0m   torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39m_record_memory_history(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mexperiment_memory_profiling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m experiment_memory_profiling(net2\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[0;32m     31\u001b[0m experiment_memory_profiling(net3\u001b[38;5;241m.\u001b[39mcuda())\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mexperiment_memory_profiling\u001b[1;34m(net, max_iter)\u001b[0m\n\u001b[0;32m      4\u001b[0m loss_fun \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# tell CUDA to start recording memory allocations\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_memory_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43menabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;66;03m# Sample random datapoint\u001b[39;00m\n\u001b[0;32m     10\u001b[0m   x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m50\u001b[39m))\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\cuda\\memory.py:715\u001b[0m, in \u001b[0;36m_record_memory_history\u001b[1;34m(enabled, *args, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _record_memory_history_legacy(enabled, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_record_memory_history_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\cuda\\memory.py:725\u001b[0m, in \u001b[0;36m_record_memory_history_impl\u001b[1;34m(enabled, context, stacks, max_entries, device)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_record_memory_history_impl\u001b[39m(\n\u001b[0;32m    719\u001b[0m     enabled: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    720\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     device: Union[Device, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    724\u001b[0m ):\n\u001b[1;32m--> 725\u001b[0m     \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_record_memory_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_entries\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: record_context_cpp is not support on non-linux non-x86_64 platforms"
     ]
    }
   ],
   "source": [
    "TIME_FORMAT_STR: str = \"%b_%d_%H_%M_%S\"\n",
    "\n",
    "def experiment_memory_profiling(net, max_iter=1000):\n",
    "  loss_fun = nn.MSELoss()\n",
    "\n",
    "  # tell CUDA to start recording memory allocations\n",
    "  torch.cuda.memory._record_memory_history(enabled='all')\n",
    "  for j in range(max_iter):\n",
    "    # Sample random datapoint\n",
    "    x = torch.randn((5,50)).cuda()\n",
    "    y = torch.randn([5]).cuda()\n",
    "\n",
    "    # Do inference\n",
    "    pred = net(x)\n",
    "    loss = loss_fun(pred, y)\n",
    "    net.backward(loss)\n",
    "\n",
    "  # save a snapshot of the memory allocations\n",
    "  timestamp = datetime.now().strftime(TIME_FORMAT_STR)\n",
    "  file_prefix = f\"{timestamp}\"\n",
    "\n",
    "  s = torch.cuda.memory._snapshot()\n",
    "  with open(f\"{file_prefix}.pickle\", \"wb\") as f:\n",
    "      dump(s, f)\n",
    "\n",
    "  # tell CUDA to stop recording memory allocations now\n",
    "  torch.cuda.memory._record_memory_history(enabled=None)\n",
    "\n",
    "experiment_memory_profiling(net1.cuda())\n",
    "experiment_memory_profiling(net2.cuda())\n",
    "experiment_memory_profiling(net3.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper on this\n",
    "https://arxiv.org/pdf/2306.09782.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
