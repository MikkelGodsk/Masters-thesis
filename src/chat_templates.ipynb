{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring chat templates\n",
    "References: https://huggingface.co/docs/transformers/chat_templating\n",
    "and https://github.com/huggingface/trl/blob/4d862da181620ccdf274138e94eff74f0c9b83be/trl/trainer/utils.py#L63\n",
    "\n",
    "`DataCollatorForCompletionOnlyLM` explanation (this one is not in the documentation): When calling the method `torch_call(examples)` of this class, it takes a list of examples (which must each be tokenized vectors of shape (n,)) and matches with the `instruction_template` and `response_template` given in its constructor... These are not easy to find, and requires to inspect the `tokenizer.default_chat_template` string.\n",
    "Then it matches the tokenized templates with the tokenized text, and splits where appropriate. *This is tricky to deal with, since tokenizers typically are context-sensitive.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\torch\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from trl.trainer import DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset_name:str=\"GAIR/lima\"\n",
    "\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "ds = load_dataset(dataset_name, 'plain_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See template using\n",
    "tokenizer_llama.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the GPT2TokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', '<pad>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'facebook/opt-125m'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_templates.new_opt_chat_template import new_opt_chat_template\n",
    "tokenizer_opt.add_special_tokens({'sep_token': '<SEP>'})\n",
    "tokenizer_opt.chat_template = new_opt_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Can brain cells move? By movement I mean long distance migration (preferably within the brain only). [/INST] The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration). </s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ds['train'][0]['conversations']\n",
    "x_formatted = [\n",
    "    {'role': 'user', 'content': x[0]},\n",
    "    {'role': 'assistant', 'content': x[1]}\n",
    "]\n",
    "tokenizer_llama.apply_chat_template(x_formatted, tokenize=False)   # This method is also what SFTTrainer calls... So I need to ensure that this is on the right format!\n",
    "\n",
    "# It does NOT seem that apply_chat_template supports the usual instruction format as they claim here: https://huggingface.co/docs/trl/sft_trainer#dataset-format-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>Can brain cells move? By movement I mean long distance migration (preferably within the brain only). <SEP> The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration). </s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_opt.apply_chat_template(x_formatted, tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def filter_example(x):\n",
    "    return x[\"source\"] != \"multi_turn\"\n",
    "\n",
    "def process_example(x, tokenizer):\n",
    "    x = x['conversations']\n",
    "    assert len(x) in [1,2], \"The multi-turn format is not supported\"\n",
    "    x_out = [{'role': 'user', 'content': x[0]}]   # Instruction\n",
    "    if len(x) == 2:\n",
    "        x_out.append({'role': 'assistant', 'content': x[1]})   # Response\n",
    "    return {'text': tokenizer.apply_chat_template(x_out, tokenize=False, add_generation_prompt=True)}  # NOTE: Calling `apply_chat_template` on the instance again is no bueno\n",
    "\n",
    "map_kwargs = {'remove_columns': [\"conversations\", \"source\"]}\n",
    "\n",
    "opt_processer = partial(process_example, tokenizer=tokenizer_opt)\n",
    "llama_processer = partial(process_example, tokenizer=tokenizer_llama)\n",
    "\n",
    "train_ds_opt = ds['train'].filter(filter_example).map(opt_processer, **map_kwargs)\n",
    "train_ds_llama = ds['train'].filter(filter_example).map(llama_processer, **map_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '</s>Can brain cells move? By movement I mean long distance migration (preferably within the brain only). <SEP> The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration). </s>'}\n",
      "\n",
      "Prompt and instruction:\n",
      "\n",
      "</s></s>Can brain cells move? By movement I mean long distance migration (preferably within the brain only). <SEP>\n",
      "\n",
      " The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\n",
      "However, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\n",
      "In  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\n",
      "Neuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\n",
      "Post-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\n",
      "Not surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration). </s>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_opt\n",
    "\n",
    "# Just picking the dataset, and instruction and response templates\n",
    "if tokenizer == tokenizer_llama:\n",
    "    instruction_template = '[INST]'\n",
    "    response_template = '[/INST]'\n",
    "    x = train_ds_llama[0]\n",
    "elif tokenizer == tokenizer_opt:\n",
    "    instruction_template = None #tokenizer_opt.bos_token\n",
    "    response_template = tokenizer_opt.sep_token\n",
    "    x = train_ds_opt[0]\n",
    "print(f\"{x}\\n\")\n",
    "\n",
    "x_tokenized = tokenizer(x['text'], return_tensors='pt')\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer, mlm=False, instruction_template=instruction_template, response_template=response_template\n",
    ")\n",
    "collated_x = collator.torch_call([x_tokenized['input_ids'][0]])\n",
    "input_ids = collated_x['input_ids']\n",
    "labels = collated_x['labels']\n",
    "\n",
    "print(\"Prompt and instruction:\\n\")\n",
    "print(f\"{tokenizer.decode(input_ids[labels == collator.ignore_index])}\\n\")\n",
    "print(f\"{tokenizer.decode(input_ids[labels != collator.ignore_index])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stress test: Can we use it without failing even once?\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "instruction_template = '[INST]'\n",
    "response_template = '[/INST]'\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer_llama, mlm=False, instruction_template=instruction_template, response_template=response_template\n",
    ")\n",
    "\n",
    "for x in train_ds_llama:\n",
    "    x_tokenized = tokenizer_llama(x['text'], return_tensors='pt')\n",
    "    collated_x = collator.torch_call([x_tokenized['input_ids'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stress test: Can we use it without failing even once?\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "instruction_template = None\n",
    "response_template = tokenizer_opt.sep_token\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer_opt, mlm=False, instruction_template=instruction_template, response_template=response_template\n",
    ")\n",
    "\n",
    "for x in train_ds_opt:\n",
    "    x_tokenized = tokenizer_opt(x['text'], return_tensors='pt')\n",
    "    collated_x = collator.torch_call([x_tokenized['input_ids'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"</s></s>I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\\n* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\\n* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\\n* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\\n* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\\nHelp me write the paper's introduction. <SEP>\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tokenizer_opt\n",
    "\n",
    "if tokenizer == tokenizer_llama:\n",
    "    instruction_template = '[INST]'\n",
    "    response_template = '[/INST]'\n",
    "elif tokenizer == tokenizer_opt:\n",
    "    instruction_template = None\n",
    "    response_template = tokenizer_opt.sep_token\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer, mlm=False, instruction_template=instruction_template, response_template=response_template\n",
    ")\n",
    "processor = partial(process_example, tokenizer=tokenizer)\n",
    "ds_test = ds['test'].filter(filter_example).map(processor, **map_kwargs)\n",
    "x = ds_test[0]\n",
    "x_tokenized = tokenizer(x['text'], return_tensors='pt')\n",
    "collated_x = collator.torch_call([x_tokenized['input_ids'][0]])\n",
    "input_ids = collated_x['input_ids']\n",
    "labels = collated_x['labels']\n",
    "tokenizer.decode(input_ids[labels == collator.ignore_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
