{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eyMGhdUfRsQ",
        "outputId": "922fad72-84a4-4c34-dc1c-5be9281b0526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m711.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m751.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m596.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/166.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:02:13\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/102.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:18\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCloning into 'geometry-of-truth'...\n",
            "remote: Enumerating objects: 273, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 273 (delta 55), reused 59 (delta 45), pack-reused 195\u001b[K\n",
            "Receiving objects: 100% (273/273), 71.49 MiB | 13.55 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n"
          ]
        }
      ],
      "source": [
        "# This setup takes roughly 15 mins to run\n",
        "#!pip install -q accelerate\n",
        "#!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
        "#!git clone https://github.com/saprmarks/geometry-of-truth.git\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm   # Change this\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "dataset_dir = 'geometry-of-truth/datasets'\n",
        "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR_MSC\")\n",
        "cache_dir = os.path.join(\n",
        "    OUTPUT_DIR, \"cache_dir\", \"huggingface\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-hf\", device_map=\"auto\", load_in_4bit=True, cache_dir=cache_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\", cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeXtuayopilR"
      },
      "outputs": [],
      "source": [
        "print(model.model)\n",
        "print(model.model.layers[0])   # Source code: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L693"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQIKdSC90Moq"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import transformers\n",
        "\n",
        "\n",
        "dtype = np.float16\n",
        "\n",
        "\n",
        "class Hook:\n",
        "  # Inspired by https://github.com/saprmarks/geometry-of-truth/blob/main/generate_acts.py\n",
        "  def __init__(self):\n",
        "    self.activations = []\n",
        "    self.idx = -1   # The index of the token we look at the internal state for\n",
        "\n",
        "  def __call__(self, module, args, output, **kwargs):\n",
        "    assert len(output) == 2 and isinstance(output[1], transformers.cache_utils.DynamicCache) and isinstance(output[0], torch.Tensor)\n",
        "    o = output[0][...,self.idx,:].detach().cpu().numpy().astype(dtype)\n",
        "    self.activations.append(o)\n",
        "\n",
        "\n",
        "def compute_activations(statements: List[str], model: torch.nn.Module) -> np.ndarray:\n",
        "  \"\"\"\n",
        "    Returns:\n",
        "    - Activations of shape [num_layers, num_samples, n_hidden_dim]\n",
        "  \"\"\"\n",
        "  hooks = []\n",
        "  handles = []\n",
        "  for i, layer in enumerate(model.model.layers):\n",
        "    hook = Hook()\n",
        "    handle = layer.register_forward_hook(hook)\n",
        "    hooks.append(hook)\n",
        "    handles.append(handle)\n",
        "\n",
        "  for statement in tqdm(statements):\n",
        "    tokens = tokenizer.encode(statement, return_tensors='pt').cuda()\n",
        "    _ = model(tokens)\n",
        "\n",
        "  for handle in handles:\n",
        "    handle.remove()\n",
        "\n",
        "  activations = []\n",
        "  for hook in hooks:\n",
        "    activations.append(np.vstack(hook.activations))\n",
        "\n",
        "  return np.stack(activations, axis=0)\n",
        "\n",
        "\n",
        "# Empty forward hooks just in case something happened.\n",
        "for layer in model.model.layers:\n",
        "  layer._forward_hooks.clear()\n",
        "  assert not len(layer._forward_hooks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py-yCsEx1s-3"
      },
      "outputs": [],
      "source": [
        "idx = 100 #50\n",
        "df = pd.read_csv(os.path.join(dataset_dir, 'common_claim_true_false.csv')) #'larger_than.csv'))   # Larger than produces obvious separability.... In most layers!!\n",
        "df_concat = pd.concat((df.loc[df['label'] == 0][0:idx], df.loc[df['label'] == 1][0:idx]))\n",
        "activations = compute_activations(df_concat['statement'], model)   # [layers, statements, np.array([1,tokens,4096])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_1WnJ0jJ8R4"
      },
      "source": [
        "Not super clear for `common_claim_true_false.csv` (log reg accuracy of 0.615 in CV), but it seems like there is some for Llama-2-13b. But looking in the paper, it also seemed like this one might not be super clear in the model.<br>\n",
        "For `larger_than.csv`, it is very clear, but then the representation seems to be present basically everywhere in the model.<br>\n",
        "With `cities.csv`, it is very clear and logistic regression gets an accuracy of 0.995 in CV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVTPhHyi8MWk"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_pca(layer, n_components):\n",
        "    X, y = activations[layer,...], df_concat['label']\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    fig, ax = plt.subplots(n_components, n_components, figsize=(20,20))\n",
        "    for x_component in range(n_components):\n",
        "        for y_component in range(n_components):\n",
        "            ax[x_component, y_component].scatter(X_pca[y==0, x_component], X_pca[y==0, y_component], label='False')\n",
        "            ax[x_component, y_component].scatter(X_pca[y==1, x_component], X_pca[y==1, y_component], label='True')\n",
        "            ax[x_component, y_component].set_title(f'x: {x_component}, y: {y_component}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C97f7rD8ExCh"
      },
      "outputs": [],
      "source": [
        "plot_pca(14, n_components=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrnboYZr3tv2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "layer = 14\n",
        "X, y = activations[layer,...], df_concat['label'].to_numpy()\n",
        "accs = []\n",
        "for train_idx, test_idx in KFold(n_splits=5).split(X, y):\n",
        "    X_train, y_train, X_test, y_test = X[train_idx,...], y[train_idx], X[test_idx,...], y[test_idx]\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    log_reg = LogisticRegression(max_iter=1000)\n",
        "    log_reg.fit(X_train, y_train)\n",
        "    y_pred = log_reg.predict(X_test)\n",
        "    accs.append(accuracy_score(y_test, y_pred))\n",
        "print(np.mean(accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpZg8S063WTd"
      },
      "outputs": [],
      "source": [
        "for layer in range(40):\n",
        "  print(f\"Layer {layer+1}\")\n",
        "  plot_pca(layer, n_components=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
