{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6HxQ68JPzw6"
      },
      "source": [
        "# Normal ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_KSJ5ZkczH_",
        "outputId": "e9ddd41f-7c78-4d25-be42-0868c2a709d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "class ANN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dense_1 = nn.LazyLinear(50)\n",
        "    self.dense_2 = nn.LazyLinear(50)\n",
        "    self.dense_3 = nn.LazyLinear(1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z0 = x\n",
        "    z1 = F.relu(self.dense_1(z0))\n",
        "    z2 = F.relu(self.dense_2(z1))\n",
        "    z3 = self.dense_3(z2)\n",
        "    return z3\n",
        "\n",
        "\n",
        "def net_eq(self, other):   # Overloading __eq__ gives issues\n",
        "  for s_param, o_param in zip(self.parameters(), other.parameters()):\n",
        "    if not torch.all(s_param == o_param):\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Test: Does deep copy work as anticipated? #\n",
        "#############################################\n",
        "for i in range(10):\n",
        "  loss_fun = nn.MSELoss()\n",
        "  x = torch.randn((1,50))\n",
        "  y = torch.randn([1])\n",
        "\n",
        "  # Inference to initialize network weights\n",
        "  net1 = ANN()\n",
        "  pred1 = net1(x)\n",
        "  net2 = deepcopy(net1)   # So they get the same parameters\n",
        "  pred2 = net2(x)\n",
        "  assert net_eq(net1, net2)\n",
        "\n",
        "  # Define optimizers differently\n",
        "  optim1 = torch.optim.Adam(net1.parameters())\n",
        "  optim2s = [\n",
        "      torch.optim.Adam(layer.parameters()) for layer in [\n",
        "          net2.dense_1, net2.dense_2, net2.dense_3\n",
        "      ]]\n",
        "\n",
        "  # Do a weight update on the first net\n",
        "  loss1 = loss_fun(pred1, y)\n",
        "  loss1.backward()\n",
        "  optim1.step()\n",
        "\n",
        "  # Do a weight update on the second net\n",
        "  loss2 = loss_fun(pred2, y)\n",
        "  loss2.backward()\n",
        "  for optim2 in reversed(optim2s):\n",
        "    optim2.step()\n",
        "\n",
        "  # See if they are the same\n",
        "  assert net_eq(net1, net2)\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# Test to be sure that the parameters are not just references of each other (otherwise it would give a false positive) #\n",
        "########################################################################################################################\n",
        "for i in range(10):\n",
        "  # Just to be entirely sure we did not just end up copying the parameters\n",
        "  # (from a failure in the deep copy), we do it again but don't update one of the layers on purpose\n",
        "  loss_fun = nn.MSELoss()\n",
        "  x = torch.randn((1,50))\n",
        "  y = torch.randn([1])\n",
        "\n",
        "  # Inference to initialize network weights\n",
        "  net1 = ANN()\n",
        "  pred1 = net1(x)\n",
        "  net2 = deepcopy(net1)   # So they get the same parameters\n",
        "  pred2 = net2(x)\n",
        "  assert net_eq(net1, net2)\n",
        "\n",
        "  # Define optimizers differently\n",
        "  optim1 = torch.optim.Adam(net1.parameters())\n",
        "  optim2s = [\n",
        "      torch.optim.Adam(layer.parameters()) for layer in [\n",
        "          net2.dense_1, net2.dense_2, net2.dense_3\n",
        "      ]]\n",
        "\n",
        "  # Do a weight update on the first net\n",
        "  loss1 = loss_fun(pred1, y)\n",
        "  loss1.backward()\n",
        "  optim1.step()\n",
        "\n",
        "  # Do a weight update on the second net\n",
        "  loss2 = loss_fun(pred2, y)\n",
        "  loss2.backward()\n",
        "  for optim2 in reversed(optim2s[1:]):\n",
        "    optim2.step()\n",
        "\n",
        "  # See if they are the same\n",
        "  assert not net_eq(net1, net2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jYr7qvkTuCi"
      },
      "source": [
        "# Manually computing the gradient across a single linear+relu layer, and comparing it with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wN5XAZbJrp6A"
      },
      "outputs": [],
      "source": [
        "#######################################################################################################################\n",
        "# Sanity check: Computing the gradient across a single layer and comparing with the chain rule (manual derivation)... #\n",
        "#######################################################################################################################\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "for i in range(10):   # Just for a sanity check...\n",
        "  B = 2\n",
        "  layer = nn.Linear(5, 3, bias=True)\n",
        "  z0 = torch.randn([B, 5], requires_grad=True, dtype=torch.float32)  # Note: This is transposed compared to the derivation in the thesis.\n",
        "  z1 = layer(z0)\n",
        "  z2 = F.relu(z1)\n",
        "\n",
        "  upstream_grad = torch.randn([B,3], dtype=torch.float32)\n",
        "  z2.backward(upstream_grad, inputs=[z0]+list(layer.parameters()))\n",
        "\n",
        "  relu_grad = (z1>0).float()  # Note: This is transposed compared to the derivation in the thesis.\n",
        "\n",
        "  # Theoretical layer weight grad\n",
        "  e = upstream_grad * relu_grad\n",
        "  assert e.shape == torch.Size([B,3])\n",
        "  weight_grad = e.T @ z0\n",
        "  assert torch.all(layer.weight.grad == weight_grad)\n",
        "\n",
        "  # Theoretical bias grad\n",
        "  bias_grad = (e.T @ torch.ones([B,1])).squeeze(1)\n",
        "  assert torch.all(layer.bias.grad == bias_grad)\n",
        "\n",
        "  # Theoretical error signal\n",
        "  downstream_grad = e @ layer.weight\n",
        "  assert torch.all(z0.grad == downstream_grad)\n",
        "\n",
        "\n",
        "\n",
        "#print((layer.weight.grad, weight_grad))\n",
        "#print((layer.bias.grad, bias_grad))\n",
        "#print((z0.grad, pass_through_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Zqz_Hvw9GH"
      },
      "source": [
        "# A proof-of-concept implementation of MEBP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zwJriuUFpvSx"
      },
      "outputs": [],
      "source": [
        "class ANN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dense_1 = nn.LazyLinear(50)\n",
        "    self.dense_2 = nn.LazyLinear(50)\n",
        "    self.dense_3 = nn.LazyLinear(1)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters())\n",
        "\n",
        "  def forward(self, x):\n",
        "    z0 = x\n",
        "    z1 = F.relu(self.dense_1(z0))\n",
        "    z2 = F.relu(self.dense_2(z1))\n",
        "    z3 = self.dense_3(z2)\n",
        "    return z3\n",
        "\n",
        "  def backward(self, loss):\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "class MemoryEfficientCopyANN(nn.Module):\n",
        "  def __init__(self, other):\n",
        "    super().__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dense_1 = deepcopy(other.dense_1) #nn.LazyLinear(50)\n",
        "    self.dense_2 = deepcopy(other.dense_2) #nn.LazyLinear(50)\n",
        "    self.dense_3 = deepcopy(other.dense_3) #nn.LazyLinear(1)\n",
        "    self.dense_1_optim = torch.optim.Adam(self.dense_1.parameters())\n",
        "    self.dense_2_optim = torch.optim.Adam(self.dense_2.parameters())\n",
        "    self.dense_3_optim = torch.optim.Adam(self.dense_3.parameters())\n",
        "    self.activations = []\n",
        "\n",
        "  def forward(self, x):\n",
        "    z0 = x\n",
        "    z1 = F.relu(self.dense_1(z0))\n",
        "    z2 = F.relu(self.dense_2(z1))\n",
        "    z3 = self.dense_3(z2)\n",
        "    self.activations = [z0,z1,z2,z3]   # Store references to tensors\n",
        "    return z3\n",
        "\n",
        "  def backward(self, loss):\n",
        "    [z0,z1,z2,z3] = self.activations\n",
        "    self.activations = []\n",
        "\n",
        "    # Target error\n",
        "    loss.backward(inputs=[z3], retain_graph=True)\n",
        "    delta_L = z3.grad\n",
        "\n",
        "    # Backpropagation with in-place optimization application and deletion\n",
        "    z3.backward(delta_L, inputs=[z2]+list(self.dense_3.parameters()), retain_graph=True)\n",
        "    delta_3 = z2.grad\n",
        "    self.dense_3_optim.step()\n",
        "    self.dense_3_optim.zero_grad()\n",
        "    assert self.dense_3.weight.grad is None\n",
        "    assert self.dense_3.bias.grad is None\n",
        "    del delta_L\n",
        "    del z3\n",
        "\n",
        "    z2.backward(delta_3, inputs=[z1]+list(self.dense_2.parameters()), retain_graph=True)\n",
        "    delta_2 = z1.grad\n",
        "    self.dense_2_optim.step()\n",
        "    self.dense_2_optim.zero_grad()\n",
        "    assert self.dense_2.weight.grad is None\n",
        "    assert self.dense_2.bias.grad is None\n",
        "    del delta_3\n",
        "    del z2\n",
        "\n",
        "    z1.backward(delta_2, inputs=list(self.dense_1.parameters()), retain_graph=True)\n",
        "    self.dense_1_optim.step()\n",
        "    self.dense_1_optim.zero_grad()\n",
        "    assert self.dense_1.weight.grad is None\n",
        "    assert self.dense_1.bias.grad is None\n",
        "    del delta_2\n",
        "    del z1\n",
        "\n",
        "\n",
        "def net_eq(self, other):   # Overloading __eq__ gives issues\n",
        "  \"\"\"\n",
        "    Defines two networks as the same if all their parameters are the same\n",
        "    regardless of which class they are...\n",
        "  \"\"\"\n",
        "  for s_param, o_param in zip(self.parameters(), other.parameters()):\n",
        "    if not torch.all(s_param == o_param):\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Test that it gives the same result as the previous ANN #\n",
        "##########################################################\n",
        "for i in range(10):\n",
        "  # Initialize networks\n",
        "  loss_fun = nn.MSELoss()\n",
        "  x = torch.randn((1,50))\n",
        "  net1 = ANN()\n",
        "  pred1 = net1(x)  # Initialize weights\n",
        "  net2 = MemoryEfficientCopyANN(net1) # They get the same parameters\n",
        "  assert net_eq(net1, net2)\n",
        "\n",
        "  for j in range(10):\n",
        "    x = torch.randn((1,50))\n",
        "    y = torch.randn([1])\n",
        "\n",
        "    # Inference and backward\n",
        "    pred1 = net1(x)\n",
        "    loss1 = loss_fun(pred1, y)\n",
        "    net1.backward(loss1)\n",
        "\n",
        "    pred2 = net2(x)\n",
        "    loss2 = loss_fun(pred2, y)\n",
        "    net2.backward(loss2)\n",
        "\n",
        "    # See if their parameters are equal\n",
        "    assert net_eq(net1, net2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTgOV78-vTpy",
        "outputId": "735b1439-6331-40e8-c8fe-fb47a04abdeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0622]])\n",
            "None\n",
            "tensor([[-0.1235,  0.0092, -0.0157,  0.1239,  0.1425, -0.0889,  0.1394, -0.0997,\n",
            "         -0.0472,  0.0039, -0.0728,  0.0582, -0.0176, -0.1152,  0.0290,  0.0760,\n",
            "         -0.0502, -0.0193, -0.1090, -0.0832, -0.0413, -0.0591,  0.1392,  0.1319,\n",
            "          0.1019, -0.1360, -0.1283,  0.0134, -0.1374,  0.0843,  0.1450,  0.1448,\n",
            "          0.0108, -0.0203,  0.1238, -0.0971, -0.1053,  0.0800, -0.0249,  0.0301,\n",
            "          0.1482, -0.1274, -0.1231,  0.0243, -0.0395, -0.0117, -0.0603,  0.0351,\n",
            "         -0.0201,  0.1259]])\n",
            "tensor([[-0.0125,  0.0138,  0.0082,  0.0052, -0.0614, -0.0092,  0.0079, -0.0165,\n",
            "          0.0175, -0.0181,  0.0129,  0.0071,  0.0183, -0.0718,  0.0019, -0.0054,\n",
            "          0.0170, -0.0150, -0.0127,  0.0065,  0.0844, -0.0414,  0.0362, -0.0496,\n",
            "          0.0241, -0.0246, -0.0027, -0.0040,  0.0093,  0.0221, -0.0219,  0.0251,\n",
            "         -0.0045,  0.0125, -0.0197, -0.0251,  0.0449,  0.0015,  0.0338, -0.0589,\n",
            "          0.0163,  0.0328,  0.0352, -0.0282, -0.0039, -0.0418,  0.0288, -0.0037,\n",
            "          0.0387,  0.0254]])\n",
            "tensor([[ 0.0107, -0.0169,  0.0030,  0.0005,  0.0215,  0.0047, -0.0054, -0.0185,\n",
            "         -0.0074,  0.0093, -0.0018, -0.0024,  0.0073, -0.0285, -0.0115, -0.0105,\n",
            "         -0.0038, -0.0010, -0.0023, -0.0034,  0.0046,  0.0159, -0.0099, -0.0075,\n",
            "         -0.0079,  0.0029,  0.0140,  0.0077,  0.0085, -0.0085, -0.0228, -0.0116,\n",
            "         -0.0256,  0.0143,  0.0020,  0.0082, -0.0067,  0.0095,  0.0060, -0.0094,\n",
            "         -0.0106, -0.0139, -0.0002,  0.0239,  0.0095,  0.0088, -0.0016,  0.0038,\n",
            "          0.0157, -0.0054]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-4733aa8bbc44>:20: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(z2.grad)\n"
          ]
        }
      ],
      "source": [
        "# Smaller test setup:\n",
        "dense_1 = nn.Linear(50,50)\n",
        "dense_2 = nn.Linear(50,50)\n",
        "dense_3 = nn.Linear(50,1)\n",
        "\n",
        "loss_fun = nn.MSELoss()\n",
        "x = torch.randn((1,50), requires_grad=True)\n",
        "y = torch.randn([1])\n",
        "\n",
        "z0 = x\n",
        "z1 = F.relu(dense_1(z0))\n",
        "z2 = F.relu(dense_2(z1))\n",
        "z3 = dense_3(z2)\n",
        "\n",
        "\n",
        "loss = loss_fun(z3, y)\n",
        "loss.backward(inputs=[z3], retain_graph=True)\n",
        "delta_L = z3.grad\n",
        "print(z3.grad)\n",
        "print(z2.grad)\n",
        "\n",
        "z3.backward(delta_L, inputs=[z2], retain_graph=True)\n",
        "delta_3 = z2.grad\n",
        "print(z2.grad)\n",
        "\n",
        "z2.backward(delta_3, inputs=[z1], retain_graph=True)\n",
        "delta_2 = z1.grad\n",
        "print(z1.grad)\n",
        "\n",
        "z1.backward(delta_2, inputs=[z0], retain_graph=True)\n",
        "print(z0.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPw0Iru6t3zd"
      },
      "source": [
        "# Testing the one suggested here (using hooks):\n",
        "https://pytorch.org/tutorials/intermediate/optimizer_step_in_backward_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aiLwoyiDwCst"
      },
      "outputs": [],
      "source": [
        "class MemoryEfficientCopyANN2(nn.Module):\n",
        "  def __init__(self, other):\n",
        "    super().__init__()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dense_1 = deepcopy(other.dense_1) #nn.LazyLinear(50)\n",
        "    self.dense_2 = deepcopy(other.dense_2) #nn.LazyLinear(50)\n",
        "    self.dense_3 = deepcopy(other.dense_3) #nn.LazyLinear(1)\n",
        "    self.setup_optimizer()\n",
        "\n",
        "  def _optimizer_hook(self, parameter) -> None:\n",
        "    self.optimizer_dict[parameter].step()\n",
        "    self.optimizer_dict[parameter].zero_grad()\n",
        "\n",
        "  def setup_optimizer(self):\n",
        "    self.optimizer_dict = {p: torch.optim.Adam([p], foreach=False) for p in self.parameters()}\n",
        "    for p in self.parameters():\n",
        "      p.register_post_accumulate_grad_hook(self._optimizer_hook)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z0 = x\n",
        "    z1 = F.relu(self.dense_1(z0))\n",
        "    z2 = F.relu(self.dense_2(z1))\n",
        "    z3 = self.dense_3(z2)\n",
        "    return z3\n",
        "\n",
        "  def backward(self, loss):\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  # Initialize networks\n",
        "  loss_fun = nn.MSELoss()\n",
        "  x = torch.randn((1,50))\n",
        "  net1 = ANN()\n",
        "  pred1 = net1(x)  # Initialize weights\n",
        "  net2 = MemoryEfficientCopyANN2(net1) # They get the same parameters\n",
        "  assert net_eq(net1, net2)\n",
        "\n",
        "  for j in range(10):\n",
        "    x = torch.randn((1,50))\n",
        "    y = torch.randn([1])\n",
        "\n",
        "    # Inference and backward\n",
        "    pred1 = net1(x)\n",
        "    loss1 = loss_fun(pred1, y)\n",
        "    net1.backward(loss1)\n",
        "\n",
        "    pred2 = net2(x)\n",
        "    loss2 = loss_fun(pred2, y)\n",
        "    net2.backward(loss2)\n",
        "\n",
        "    # See if their parameters are equal\n",
        "    assert net_eq(net1, net2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BkvBhiIOu_G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "R6HxQ68JPzw6",
        "8jYr7qvkTuCi",
        "l_Zqz_Hvw9GH",
        "gPw0Iru6t3zd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
